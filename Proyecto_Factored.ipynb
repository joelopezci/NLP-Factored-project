{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Proyecto Factored.ipynb","provenance":[{"file_id":"1vWrDzN1E0HefisiBLWJ5hDHsJrZy1swd","timestamp":1587139598677}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"G_pO4HMRum1l"},"source":["#**LIBRERÍAS, PERMISOS Y LECTURA DE DATOS**"]},{"cell_type":"code","metadata":{"id":"ZTMNAGq8naQs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618082174589,"user_tz":300,"elapsed":40828,"user":{"displayName":"Johan Estiben Lopez Cifuentes","photoUrl":"","userId":"00114855385701678854"}},"outputId":"e5195ee3-88e7-4e69-ed45-7a3eef7d9179"},"source":["import numpy as np\n","import pandas as pd \n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jDNcSTRqptpr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618082194639,"user_tz":300,"elapsed":6037,"user":{"displayName":"Johan Estiben Lopez Cifuentes","photoUrl":"","userId":"00114855385701678854"}},"outputId":"c879b3ad-5fcd-48f7-e3b6-29614d5e00b4"},"source":["# Importación librerías para preprocesamiento \n","import re  #Regular expression operations = RE\n","\n","# STOPWORDS\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","#PUNCTUATION\n","import string\n","import spacy # spaCy is a leading NLP library when working with text.\n","nlp = spacy.load(\"en_core_web_sm\") # Loading English Language Model\n","from spacy import lemmatizer\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i1RiFTqRp7qF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618082250598,"user_tz":300,"elapsed":47153,"user":{"displayName":"Johan Estiben Lopez Cifuentes","photoUrl":"","userId":"00114855385701678854"}},"outputId":"7b0f090c-5267-42f0-ebf2-876421e053ff"},"source":["!pip install tensorflow==2.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/5c/f1d66de5dde6f3ff528f6ea1fd0757a0e594d17debb3ec7f82daa967ea9a/tensorflow-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (86.3MB)\n","\u001b[K     |████████████████████████████████| 86.3MB 51kB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (3.12.4)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (1.32.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (0.8.1)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (1.19.5)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (1.15.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (1.1.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (3.3.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (0.12.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (0.2.0)\n","Collecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.2MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (0.36.2)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Collecting tensorflow-estimator<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n","\u001b[K     |████████████████████████████████| 450kB 46.4MB/s \n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (1.12.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (1.1.0)\n","Collecting tensorboard<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 46.6MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==2.0) (54.2.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0) (2.10.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.3.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.23.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.3)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.28.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.8.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2020.12.5)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.2.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.2.8)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.4.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.8)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=f254f38d836e2215b9d632e5fdff37bfe6531bf1c260d873daf3181262412913\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: keras-applications, gast, tensorflow-estimator, tensorboard, tensorflow\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","  Found existing installation: tensorflow-estimator 2.4.0\n","    Uninstalling tensorflow-estimator-2.4.0:\n","      Successfully uninstalled tensorflow-estimator-2.4.0\n","  Found existing installation: tensorboard 2.4.1\n","    Uninstalling tensorboard-2.4.1:\n","      Successfully uninstalled tensorboard-2.4.1\n","  Found existing installation: tensorflow 2.4.1\n","    Uninstalling tensorflow-2.4.1:\n","      Successfully uninstalled tensorflow-2.4.1\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tdEWHt9RqSUA"},"source":["#import tensorflow as tf\n","import tensorflow_hub as hub"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXP31C8Docqs"},"source":["data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Factored project/data/train.csv')\n","data = data.drop(columns=['id','keyword','location'], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Ibdh6iXpBG5"},"source":["# **NUEVO DATAFRAME**\n","\n","CREACIÓN DE NUEVO DATAFRAME PARA NO MANIPULAR EL ORIGINAL\n","\n","***COLUMNAS:***\n","\n","\n","*   **text**: Tiene el texto tal cual el dataset\n","*   **target**: target del dataset \n","*   **NoRuido**: Todo el ruido eliminado de la columna **text** (URL's, HTML tags, Emojis)\n","*   **PRE**: Preprocesamiento a **text**\n","*   **URLS**: Sólo se elimina las URL's de **text**\n","*   **HTML**: Sólo se elimina las HTML tags de **text**\n","*   **EMOJIS**: Sólo se elimina los emojis de **text**\n","*   **TODO**:Se hace preprocesamiento y se elimina todo el ruido de   **text**  \n","\n","Todo esto será hecho por la función New_dataframe\n","\n","***Nota:***\n","Puede omitir la ejecución de esta función simplemente leyendo el archivo newdata.csv con la instrucción dos celdas abajo."]},{"cell_type":"code","metadata":{"id":"ozf-_o6ZpAL_"},"source":["def New_dataframe(data): \n","  \n","  newdata = pd.DataFrame(columns=('text', 'target','NoRuido','PRE','URLS','HTML','EMOJIS','TODO'))\n","  pd.set_option('display.max_colwidth',-1)\n","  newdata['text']=data['text']\n","  newdata['target']=data['target']\n","\n","  def clean(tweet): \n","    tweet=tweet.lower()\n","    # Contractions\n","    tweet = re.sub(r\"he's\", \"he is\", tweet)\n","    tweet = re.sub(r\"there's\", \"there is\", tweet)\n","    tweet = re.sub(r\"We're\", \"We are\", tweet)\n","    tweet = re.sub(r\"That's\", \"That is\", tweet)\n","    tweet = re.sub(r\"won't\", \"will not\", tweet)\n","    tweet = re.sub(r\"they're\", \"they are\", tweet)\n","    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n","    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n","    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n","    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n","    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n","    tweet = re.sub(r\"What's\", \"What is\", tweet)\n","    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n","    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n","    tweet = re.sub(r\"There's\", \"There is\", tweet)\n","    tweet = re.sub(r\"He's\", \"He is\", tweet)\n","    tweet = re.sub(r\"It's\", \"It is\", tweet)\n","    tweet = re.sub(r\"You're\", \"You are\", tweet)\n","    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n","    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n","    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n","    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n","    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n","    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n","    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n","    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n","    tweet = re.sub(r\"you've\", \"you have\", tweet)\n","    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n","    tweet = re.sub(r\"we're\", \"we are\", tweet)\n","    tweet = re.sub(r\"what's\", \"what is\", tweet)\n","    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n","    tweet = re.sub(r\"we've\", \"we have\", tweet)\n","    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n","    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n","    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n","    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n","    tweet = re.sub(r\"who's\", \"who is\", tweet)\n","    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n","    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n","    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n","    tweet = re.sub(r\"would've\", \"would have\", tweet)\n","    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n","    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n","    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n","    tweet = re.sub(r\"We've\", \"We have\", tweet)\n","    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n","    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n","    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n","    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n","    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n","    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n","    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n","    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n","    tweet = re.sub(r\"they've\", \"they have\", tweet)\n","    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n","    tweet = re.sub(r\"should've\", \"should have\", tweet)\n","    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n","    tweet = re.sub(r\"where's\", \"where is\", tweet)\n","    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n","    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n","    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n","    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n","    tweet = re.sub(r\"They're\", \"They are\", tweet)\n","    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n","    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n","    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n","    tweet = re.sub(r\"let's\", \"let us\", tweet)\n","    tweet = re.sub(r\"it's\", \"it is\", tweet)\n","    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n","    tweet = re.sub(r\"don't\", \"do not\", tweet)\n","    tweet = re.sub(r\"you're\", \"you are\", tweet)\n","    tweet = re.sub(r\"i've\", \"I have\", tweet)\n","    tweet = re.sub(r\"that's\", \"that is\", tweet)\n","    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n","    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n","    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n","    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n","    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n","    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n","    tweet = re.sub(r\"I've\", \"I have\", tweet)\n","    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n","    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n","    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n","    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n","    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n","    tweet = re.sub(r\"It's\", \"It is\", tweet)\n","    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n","    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n","    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n","    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n","    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n","    return tweet\n","\n","  newdata['PRE'] = newdata['text'].apply(lambda s : clean(s))\n","\n","  abbreviations = {\n","    \"$\" : \" dollar \",\n","    \"€\" : \" euro \",\n","    \"4ao\" : \"for adults only\",\n","    \"a.m\" : \"before midday\",\n","    \"a3\" : \"anytime anywhere anyplace\",\n","    \"aamof\" : \"as a matter of fact\",\n","    \"acct\" : \"account\",\n","    \"adih\" : \"another day in hell\",\n","    \"afaic\" : \"as far as i am concerned\",\n","    \"afaict\" : \"as far as i can tell\",\n","    \"afaik\" : \"as far as i know\",\n","    \"afair\" : \"as far as i remember\",\n","    \"afk\" : \"away from keyboard\",\n","    \"app\" : \"application\",\n","    \"approx\" : \"approximately\",\n","    \"apps\" : \"applications\",\n","    \"asap\" : \"as soon as possible\",\n","    \"asl\" : \"age, sex, location\",\n","    \"atk\" : \"at the keyboard\",\n","    \"ave.\" : \"avenue\",\n","    \"aymm\" : \"are you my mother\",\n","    \"ayor\" : \"at your own risk\", \n","    \"b&b\" : \"bed and breakfast\",\n","    \"b+b\" : \"bed and breakfast\",\n","    \"b.c\" : \"before christ\",\n","    \"b2b\" : \"business to business\",\n","    \"b2c\" : \"business to customer\",\n","    \"b4\" : \"before\",\n","    \"b4n\" : \"bye for now\",\n","    \"b@u\" : \"back at you\",\n","    \"bae\" : \"before anyone else\",\n","    \"bak\" : \"back at keyboard\",\n","    \"bbbg\" : \"bye bye be good\",\n","    \"bbc\" : \"british broadcasting corporation\",\n","    \"bbias\" : \"be back in a second\",\n","    \"bbl\" : \"be back later\",\n","    \"bbs\" : \"be back soon\",\n","    \"be4\" : \"before\",\n","    \"bfn\" : \"bye for now\",\n","    \"blvd\" : \"boulevard\",\n","    \"bout\" : \"about\",\n","    \"brb\" : \"be right back\",\n","    \"bros\" : \"brothers\",\n","    \"brt\" : \"be right there\",\n","    \"bsaaw\" : \"big smile and a wink\",\n","    \"btw\" : \"by the way\",\n","    \"bwl\" : \"bursting with laughter\",\n","    \"c/o\" : \"care of\",\n","    \"cet\" : \"central european time\",\n","    \"cf\" : \"compare\",\n","    \"cia\" : \"central intelligence agency\",\n","    \"csl\" : \"can not stop laughing\",\n","    \"cu\" : \"see you\",\n","    \"cul8r\" : \"see you later\",\n","    \"cv\" : \"curriculum vitae\",\n","    \"cwot\" : \"complete waste of time\",\n","    \"cya\" : \"see you\",\n","    \"cyt\" : \"see you tomorrow\",\n","    \"dae\" : \"does anyone else\",\n","    \"dbmib\" : \"do not bother me i am busy\",\n","    \"diy\" : \"do it yourself\",\n","    \"dm\" : \"direct message\",\n","    \"dwh\" : \"during work hours\",\n","    \"e123\" : \"easy as one two three\",\n","    \"eet\" : \"eastern european time\",\n","    \"eg\" : \"example\",\n","    \"embm\" : \"early morning business meeting\",\n","    \"encl\" : \"enclosed\",\n","    \"encl.\" : \"enclosed\",\n","    \"etc\" : \"and so on\",\n","    \"faq\" : \"frequently asked questions\",\n","    \"fawc\" : \"for anyone who cares\",\n","    \"fb\" : \"facebook\",\n","    \"fc\" : \"fingers crossed\",\n","    \"fig\" : \"figure\",\n","    \"fimh\" : \"forever in my heart\", \n","    \"ft.\" : \"feet\",\n","    \"ft\" : \"featuring\",\n","    \"ftl\" : \"for the loss\",\n","    \"ftw\" : \"for the win\",\n","    \"fwiw\" : \"for what it is worth\",\n","    \"fyi\" : \"for your information\",\n","    \"g9\" : \"genius\",\n","    \"gahoy\" : \"get a hold of yourself\",\n","    \"gal\" : \"get a life\",\n","    \"gcse\" : \"general certificate of secondary education\",\n","    \"gfn\" : \"gone for now\",\n","    \"gg\" : \"good game\",\n","    \"gl\" : \"good luck\",\n","    \"glhf\" : \"good luck have fun\",\n","    \"gmt\" : \"greenwich mean time\",\n","    \"gmta\" : \"great minds think alike\",\n","    \"gn\" : \"good night\",\n","    \"g.o.a.t\" : \"greatest of all time\",\n","    \"goat\" : \"greatest of all time\",\n","    \"goi\" : \"get over it\",\n","    \"gps\" : \"global positioning system\",\n","    \"gr8\" : \"great\",\n","    \"gratz\" : \"congratulations\",\n","    \"gyal\" : \"girl\",\n","    \"h&c\" : \"hot and cold\",\n","    \"hp\" : \"horsepower\",\n","    \"hr\" : \"hour\",\n","    \"hrh\" : \"his royal highness\",\n","    \"ht\" : \"height\",\n","    \"ibrb\" : \"i will be right back\",\n","    \"ic\" : \"i see\",\n","    \"icq\" : \"i seek you\",\n","    \"icymi\" : \"in case you missed it\",\n","    \"idc\" : \"i do not care\",\n","    \"idgadf\" : \"i do not give a damn fuck\",\n","    \"idgaf\" : \"i do not give a fuck\",\n","    \"idk\" : \"i do not know\",\n","    \"ie\" : \"that is\",\n","    \"i.e\" : \"that is\",\n","    \"ifyp\" : \"i feel your pain\",\n","    \"IG\" : \"instagram\",\n","    \"iirc\" : \"if i remember correctly\",\n","    \"ilu\" : \"i love you\",\n","    \"ily\" : \"i love you\",\n","    \"imho\" : \"in my humble opinion\",\n","    \"imo\" : \"in my opinion\",\n","    \"imu\" : \"i miss you\",\n","    \"iow\" : \"in other words\",\n","    \"irl\" : \"in real life\",\n","    \"j4f\" : \"just for fun\",\n","    \"jic\" : \"just in case\",\n","    \"jk\" : \"just kidding\",\n","    \"jsyk\" : \"just so you know\",\n","    \"l8r\" : \"later\",\n","    \"lb\" : \"pound\",\n","    \"lbs\" : \"pounds\",\n","    \"ldr\" : \"long distance relationship\",\n","    \"lmao\" : \"laugh my ass off\",\n","    \"lmfao\" : \"laugh my fucking ass off\",\n","    \"lol\" : \"laughing out loud\",\n","    \"ltd\" : \"limited\",\n","    \"ltns\" : \"long time no see\",\n","    \"m8\" : \"mate\",\n","    \"mf\" : \"motherfucker\",\n","    \"mfs\" : \"motherfuckers\",\n","    \"mfw\" : \"my face when\",\n","    \"mofo\" : \"motherfucker\",\n","    \"mph\" : \"miles per hour\",\n","    \"mr\" : \"mister\",\n","    \"mrw\" : \"my reaction when\",\n","    \"ms\" : \"miss\",\n","    \"mte\" : \"my thoughts exactly\",\n","    \"nagi\" : \"not a good idea\",\n","    \"nbc\" : \"national broadcasting company\",\n","    \"nbd\" : \"not big deal\",\n","    \"nfs\" : \"not for sale\",\n","    \"ngl\" : \"not going to lie\",\n","    \"nhs\" : \"national health service\",\n","    \"nrn\" : \"no reply necessary\",\n","    \"nsfl\" : \"not safe for life\",\n","    \"nsfw\" : \"not safe for work\",\n","    \"nth\" : \"nice to have\",\n","    \"nvr\" : \"never\",\n","    \"nyc\" : \"new york city\",\n","    \"oc\" : \"original content\",\n","    \"og\" : \"original\",\n","    \"ohp\" : \"overhead projector\",\n","    \"oic\" : \"oh i see\",\n","    \"omdb\" : \"over my dead body\",\n","    \"omg\" : \"oh my god\",\n","    \"omw\" : \"on my way\",\n","    \"p.a\" : \"per annum\",\n","    \"p.m\" : \"after midday\",\n","    \"pm\" : \"prime minister\",\n","    \"poc\" : \"people of color\",\n","    \"pov\" : \"point of view\",\n","    \"pp\" : \"pages\",\n","    \"ppl\" : \"people\",\n","    \"prw\" : \"parents are watching\",\n","    \"ps\" : \"postscript\",\n","    \"pt\" : \"point\",\n","    \"ptb\" : \"please text back\",\n","    \"pto\" : \"please turn over\",\n","    \"qpsa\" : \"what happens\", #\"que pasa\",\n","    \"ratchet\" : \"rude\",\n","    \"rbtl\" : \"read between the lines\",\n","    \"rlrt\" : \"real life retweet\", \n","    \"rofl\" : \"rolling on the floor laughing\",\n","    \"roflol\" : \"rolling on the floor laughing out loud\",\n","    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n","    \"rt\" : \"retweet\",\n","    \"ruok\" : \"are you ok\",\n","    \"sfw\" : \"safe for work\",\n","    \"sk8\" : \"skate\",\n","    \"smh\" : \"shake my head\",\n","    \"sq\" : \"square\",\n","    \"srsly\" : \"seriously\", \n","    \"ssdd\" : \"same stuff different day\",\n","    \"tbh\" : \"to be honest\",\n","    \"tbs\" : \"tablespooful\",\n","    \"tbsp\" : \"tablespooful\",\n","    \"tfw\" : \"that feeling when\",\n","    \"thks\" : \"thank you\",\n","    \"tho\" : \"though\",\n","    \"thx\" : \"thank you\",\n","    \"tia\" : \"thanks in advance\",\n","    \"til\" : \"today i learned\",\n","    \"tl;dr\" : \"too long i did not read\",\n","    \"tldr\" : \"too long i did not read\",\n","    \"tmb\" : \"tweet me back\",\n","    \"tntl\" : \"trying not to laugh\",\n","    \"ttyl\" : \"talk to you later\",\n","    \"u\" : \"you\",\n","    \"u2\" : \"you too\",\n","    \"u4e\" : \"yours for ever\",\n","    \"utc\" : \"coordinated universal time\",\n","    \"w/\" : \"with\",\n","    \"w/o\" : \"without\",\n","    \"w8\" : \"wait\",\n","    \"wassup\" : \"what is up\",\n","    \"wb\" : \"welcome back\",\n","    \"wtf\" : \"what the fuck\",\n","    \"wtg\" : \"way to go\",\n","    \"wtpa\" : \"where the party at\",\n","    \"wuf\" : \"where are you from\",\n","    \"wuzup\" : \"what is up\",\n","    \"wywh\" : \"wish you were here\",\n","    \"yd\" : \"yard\",\n","    \"ygtr\" : \"you got that right\",\n","    \"ynk\" : \"you never know\",\n","    \"zzz\" : \"sleeping bored and tired\"}\n","\n","  def convert_abbrev(word):\n","    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n","  newdata['PRE'] = newdata['PRE'].apply(lambda s : convert_abbrev(s))\n","\n","  STOPWORDS = set(stopwords.words('english'))\n","  def text_preprocessing(text):\n","    text= text.lower() \n","    text = \" \".join([word for word in str(text).split() if word not in STOPWORDS]) \n","    return text\n","  newdata['PRE'] = newdata['PRE'].apply(lambda s : text_preprocessing(s))\n","\n","  def multiple_letters(text):\n","    text = re.sub(r'([a-zA-Z])\\1{1,}', r'\\1\\1', text)\n","    return text\n","  newdata['PRE'] = newdata['PRE'].apply(lambda s : multiple_letters(s))\n"," \n","  PUNCT_TO_REMOVE = string.punctuation\n","  def remove_punctuation(text):\n","    \"\"\"custom function to remove the punctuation\"\"\"\n","    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n","  newdata['PRE'] = newdata['PRE'].apply(lambda s : remove_punctuation(s))\n","\n","\n","\n","#ELIMINACIÓN DEL RUIDO DE TEXTO\n","\n","#   ELIMINAR URL'S DE LOS TWEETS\n","  def remove_URL(text):\n","    url = re.compile(r'https?://\\S+|www\\.\\S+')\n","    return url.sub(r'',text)\n","  newdata['URLS']=newdata['text'].apply(lambda x : remove_URL(x))\n","  \n","#   REMOVING HTML TAGS  \n","  def remove_html(text):\n","    html=re.compile(r'<.*?>')\n","    return html.sub(r'',text)\n","  newdata['HTML']=newdata['text'].apply(lambda x : remove_html(x)) \n","\n","#   REMOVING EMOJIS  \n","  def remove_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                            u\"\\U00002702-\\U000027B0\"\n","                            u\"\\U000024C2-\\U0001F251\"\n","                            \"]+\", flags=re.UNICODE)\n","    return emoji_pattern.sub(r'', text)\n","\n","  newdata['EMOJIS']=newdata['text'].apply(lambda x: remove_emoji(x))\n","\n","#   REMOVING ALL\n","  newdata['NoRuido']=newdata['text'].apply(lambda x : remove_URL(x)) \n","  newdata['NoRuido']=newdata['NoRuido'].apply(lambda x : remove_html(x)) \n","  newdata['NoRuido']=newdata['NoRuido'].apply(lambda x: remove_emoji(x))\n","\n","#   TODO\n","  newdata['TODO'] = newdata['PRE']\n","  newdata['TODO'] = newdata['TODO'].apply(lambda x : remove_URL(x)) \n","  newdata['TODO'] = newdata['TODO'].apply(lambda x : remove_html(x)) \n","  newdata['TODO'] = newdata['TODO'].apply(lambda x: remove_emoji(x))\n","\n","  newdata.to_csv('/content/drive/My Drive/Colab Notebooks/Factored project/models/newdata.csv')\n","  \n","  return newdata"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"waMfe6nGtOgg","colab":{"base_uri":"https://localhost:8080/","height":649},"executionInfo":{"status":"ok","timestamp":1618082769975,"user_tz":300,"elapsed":2103,"user":{"displayName":"Johan Estiben Lopez Cifuentes","photoUrl":"","userId":"00114855385701678854"}},"outputId":"ce8595e7-a521-4a2a-e345-e35378426359"},"source":["newdata = New_dataframe(data)\n","newdata.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n","  after removing the cwd from sys.path.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>NoRuido</th>\n","      <th>PRE</th>\n","      <th>URLS</th>\n","      <th>HTML</th>\n","      <th>EMOJIS</th>\n","      <th>TODO</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n","      <td>1</td>\n","      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n","      <td>deeds reason earthquake may allah forgive us</td>\n","      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n","      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n","      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n","      <td>deeds reason earthquake may allah forgive us</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>forest fire near la ronge sask canada</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>forest fire near la ronge sask canada</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n","      <td>1</td>\n","      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n","      <td>residents asked shelter place notified officers evacuation shelter place orders expected</td>\n","      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n","      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n","      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n","      <td>residents asked shelter place notified officers evacuation shelter place orders expected</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>13,000 people receive #wildfires evacuation orders in California</td>\n","      <td>1</td>\n","      <td>13,000 people receive #wildfires evacuation orders in California</td>\n","      <td>13000 people receive wildfires evacuation orders california</td>\n","      <td>13,000 people receive #wildfires evacuation orders in California</td>\n","      <td>13,000 people receive #wildfires evacuation orders in California</td>\n","      <td>13,000 people receive #wildfires evacuation orders in California</td>\n","      <td>13000 people receive wildfires evacuation orders california</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n","      <td>1</td>\n","      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n","      <td>got sent photo ruby alaska smoke wildfires pours school</td>\n","      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n","      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n","      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n","      <td>got sent photo ruby alaska smoke wildfires pours school</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                                                                    text  ...                                                                                      TODO\n","0  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all                                                                  ...  deeds reason earthquake may allah forgive us                                            \n","1  Forest fire near La Ronge Sask. Canada                                                                                                 ...  forest fire near la ronge sask canada                                                   \n","2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected  ...  residents asked shelter place notified officers evacuation shelter place orders expected\n","3  13,000 people receive #wildfires evacuation orders in California                                                                       ...  13000 people receive wildfires evacuation orders california                             \n","4  Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school                                                ...  got sent photo ruby alaska smoke wildfires pours school                                 \n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"cGzEaLvX-jOx"},"source":["# # Si ha ejecutado la función New_dataframe, no es necesario ejecutar esta celda.\n","# newdata = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Factored project/models/newdata.csv')\n","# newdata.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xxbHdYqhvURB"},"source":["#**MODELOS**\n"]},{"cell_type":"code","metadata":{"id":"alKb_y4CvfCl"},"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","\n","model_1 = LogisticRegression(max_iter=500, solver='saga', random_state=40)\n","model_2 = SVC(random_state=40)\n","model_3 = RandomForestClassifier(n_estimators=100, n_jobs=2, random_state=40)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vExfJekWwUCM"},"source":["##**EMBEDDINGS**:\n","\n","\n","1.   **Bag of words**\n","2.   **Tf idf**\n","3.   **Tensorflow_hub**\n","\n","Los tres los hace la siguiente función:\n","\n"]},{"cell_type":"code","metadata":{"id":"BczZ-8Qfwu7A"},"source":["def Embedding_function(X, embedd=0):\n","  # embedd es el tipo de embeddin, puede ser:\n","  # embedd = 0 es bag of words\n","  # embedd = 1 es Tf Idf\n","  # embedd = 2 es tensorflow hub \n","  # Retorna un array \n","  from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","  v_1 = CountVectorizer()\n","  v_2 = TfidfVectorizer()\n","  if embedd==0:\n","    X = v_1.fit_transform(X)\n","    X = X.toarray()\n","  elif embedd==1:\n","    X = v_2.fit_transform(X)\n","    X = X.toarray()\n","  else:\n","    import tensorflow_hub as hub\n","    embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n","    X = embed(X)\n","    X = X.numpy()\n","  return X"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GzAU3s5zxuoY"},"source":["##**RESULTADOS**"]},{"cell_type":"code","metadata":{"id":"3pancZs_xze4"},"source":["def Results_RL(newdata):\n","  # i= 0 BOW\n","  # i=1 TF IDF \n","  # i=2 Tensor Flow hub\n","  #Partición de los datos\n","  Y_n = newdata['target']\n","  X_n = newdata.drop(columns=['target'], axis=1)\n","  from sklearn.model_selection import train_test_split\n","  from sklearn.metrics import  f1_score\n","  #Guardar modelos\n","  from sklearn.externals import joblib\n","  d_RL ={'Embedding':['BOW', 'TF IDF', 'Tensorflow hub']}\n","  for j in X_n.columns:\n","    L_RL = []\n","    for i in range(3):\n","      X = Embedding_function(X_n.loc[:,j], embedd=i)\n","      X_e, X_p, y_e, y_p = train_test_split(X, Y_n, test_size= 0.2, random_state=40)\n","      model_1.fit(X_e, y_e)\n","      y_1 = model_1.predict(X_p)\n","      var_1 = 'model_1_t{}_{}'.format(j, i)\n","      joblib.dump(model_1, '/content/drive/My Drive/Colab Notebooks/Factored project/models/{}.pkl'.format(var_1))\n","      L_RL.append(f1_score(y_p, y_1))\n","    d_RL[j] = L_RL\n","  return pd.DataFrame(data=d_RL)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XihWnWrCYi5-"},"source":["def Results_SVM(newdata):\n","  # i= 0 BOW\n","  # i=1 TF IDF \n","  # i=2 Tensor Flow hub\n","  #Partición de los datos\n","  Y_n = newdata['target']\n","  X_n = newdata.drop(columns=['target'], axis=1)\n","  from sklearn.model_selection import train_test_split\n","  from sklearn.metrics import  f1_score\n","  #Guardar modelos\n","  from sklearn.externals import joblib\n","  d_SVM ={'Embedding':['BOW', 'TF IDF', 'Tensorflow hub']}\n","  for j in X_n.columns:\n","    L_SVM = []\n","    for i in range(3):\n","      X = Embedding_function(X_n.loc[:, j], embedd=i)\n","      X_e, X_p, y_e, y_p = train_test_split(X, Y_n, test_size= 0.2, random_state=40)\n","      model_2.fit(X_e, y_e)\n","      y_2 = model_2.predict(X_p)\n","      var_2 = 'model_2_t{}_{}'.format(j,i)\n","      joblib.dump(model_2, '/content/drive/My Drive/Colab Notebooks/Factored project/models/{}.pkl'.format(var_2))\n","      L_SVM.append(f1_score(y_p, y_2))\n","    d_SVM[j] = L_SVM\n","  return pd.DataFrame(data=d_SVM)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"32pqe9QkYkI3"},"source":["def Results_RFC(newdata):\n","  # i= 0 BOW\n","  # i=1 TF IDF \n","  # i=2 Tensor Flow hub\n","  #Partición de los datos\n","  Y_n = newdata['target']\n","  X_n = newdata.drop(columns=['target'], axis=1)\n","  from sklearn.model_selection import train_test_split\n","  from sklearn.metrics import  f1_score\n","  #Guardar modelos\n","  from sklearn.externals import joblib\n","  d_RFC ={'Embedding':['BOW', 'TF IDF', 'Tensorflow hub']}\n","  for j in X_n.columns:\n","    L_RFC = []\n","    for i in range(3):\n","      X = Embedding_function(X_n.loc[:, j], embedd=i)\n","      X_e, X_p, y_e, y_p = train_test_split(X, Y_n, test_size= 0.2, random_state=40)\n","      model_3.fit(X_e, y_e)\n","      y_3 = model_3.predict(X_p)\n","      var_3 = 'model_3_t{}_{}'.format(j,i)\n","      joblib.dump(model_3, '/content/drive/My Drive/Colab Notebooks/Factored project/models/{}.pkl'.format(var_3))\n","      L_RFC.append(f1_score(y_p, y_3))\n","    d_RFC[j] = L_RFC\n","  return  pd.DataFrame(data=d_RFC)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DX12VxikJK4e"},"source":["Todos los modelos seran comparados usando la métrica **F1**. "]},{"cell_type":"code","metadata":{"id":"PSi29f63JUst","colab":{"base_uri":"https://localhost:8080/","height":196},"executionInfo":{"status":"ok","timestamp":1618084581848,"user_tz":300,"elapsed":1672213,"user":{"displayName":"Johan Estiben Lopez Cifuentes","photoUrl":"","userId":"00114855385701678854"}},"outputId":"e6f216ec-7685-441b-cd1f-91a2ab36eab7"},"source":["LR = Results_RL(newdata)\n","LR.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Embedding</th>\n","      <th>text</th>\n","      <th>NoRuido</th>\n","      <th>PRE</th>\n","      <th>URLS</th>\n","      <th>HTML</th>\n","      <th>EMOJIS</th>\n","      <th>TODO</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>BOW</td>\n","      <td>0.770465</td>\n","      <td>0.767886</td>\n","      <td>0.758509</td>\n","      <td>0.767886</td>\n","      <td>0.770465</td>\n","      <td>0.770465</td>\n","      <td>0.758509</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TF IDF</td>\n","      <td>0.751055</td>\n","      <td>0.754181</td>\n","      <td>0.738832</td>\n","      <td>0.754181</td>\n","      <td>0.751055</td>\n","      <td>0.751055</td>\n","      <td>0.738832</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Tensorflow hub</td>\n","      <td>0.782541</td>\n","      <td>0.779688</td>\n","      <td>0.765759</td>\n","      <td>0.779688</td>\n","      <td>0.782541</td>\n","      <td>0.782541</td>\n","      <td>0.765759</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        Embedding      text   NoRuido  ...      HTML    EMOJIS      TODO\n","0  BOW             0.770465  0.767886  ...  0.770465  0.770465  0.758509\n","1  TF IDF          0.751055  0.754181  ...  0.751055  0.751055  0.738832\n","2  Tensorflow hub  0.782541  0.779688  ...  0.782541  0.782541  0.765759\n","\n","[3 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"bxtqgGAdZ_Jt","colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1618103159030,"user_tz":300,"elapsed":7116308,"user":{"displayName":"Johan Estiben Lopez Cifuentes","photoUrl":"","userId":"00114855385701678854"}},"outputId":"e53c90f1-146d-47aa-9c4a-b0374bf59ce4"},"source":["SVM = Results_SVM(newdata)\n","SVM.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Embedding</th>\n","      <th>text</th>\n","      <th>NoRuido</th>\n","      <th>PRE</th>\n","      <th>URLS</th>\n","      <th>HTML</th>\n","      <th>EMOJIS</th>\n","      <th>TODO</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>BOW</td>\n","      <td>0.739796</td>\n","      <td>0.744226</td>\n","      <td>0.749153</td>\n","      <td>0.744226</td>\n","      <td>0.739796</td>\n","      <td>0.739796</td>\n","      <td>0.749153</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TF IDF</td>\n","      <td>0.746781</td>\n","      <td>0.743302</td>\n","      <td>0.730803</td>\n","      <td>0.743302</td>\n","      <td>0.746781</td>\n","      <td>0.746781</td>\n","      <td>0.730803</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Tensorflow hub</td>\n","      <td>0.786992</td>\n","      <td>0.785366</td>\n","      <td>0.785829</td>\n","      <td>0.785366</td>\n","      <td>0.786992</td>\n","      <td>0.786992</td>\n","      <td>0.785829</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        Embedding      text   NoRuido  ...      HTML    EMOJIS      TODO\n","0  BOW             0.739796  0.744226  ...  0.739796  0.739796  0.749153\n","1  TF IDF          0.746781  0.743302  ...  0.746781  0.746781  0.730803\n","2  Tensorflow hub  0.786992  0.785366  ...  0.786992  0.786992  0.785829\n","\n","[3 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"e6ZeU6OSaLvR","colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1618105991233,"user_tz":300,"elapsed":2105186,"user":{"displayName":"Johan Estiben Lopez Cifuentes","photoUrl":"","userId":"00114855385701678854"}},"outputId":"90f01093-be38-44b3-8a8c-815e51f0d508"},"source":["RFC = Results_RFC(newdata)\n","RFC.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Embedding</th>\n","      <th>text</th>\n","      <th>NoRuido</th>\n","      <th>PRE</th>\n","      <th>URLS</th>\n","      <th>HTML</th>\n","      <th>EMOJIS</th>\n","      <th>TODO</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>BOW</td>\n","      <td>0.722320</td>\n","      <td>0.734908</td>\n","      <td>0.731070</td>\n","      <td>0.734908</td>\n","      <td>0.722320</td>\n","      <td>0.722320</td>\n","      <td>0.731070</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TF IDF</td>\n","      <td>0.708520</td>\n","      <td>0.732517</td>\n","      <td>0.725237</td>\n","      <td>0.732517</td>\n","      <td>0.708520</td>\n","      <td>0.708520</td>\n","      <td>0.725237</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Tensorflow hub</td>\n","      <td>0.761269</td>\n","      <td>0.765101</td>\n","      <td>0.742181</td>\n","      <td>0.765101</td>\n","      <td>0.761269</td>\n","      <td>0.761269</td>\n","      <td>0.742181</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        Embedding      text   NoRuido  ...      HTML    EMOJIS      TODO\n","0  BOW             0.722320  0.734908  ...  0.722320  0.722320  0.731070\n","1  TF IDF          0.708520  0.732517  ...  0.708520  0.708520  0.725237\n","2  Tensorflow hub  0.761269  0.765101  ...  0.761269  0.761269  0.742181\n","\n","[3 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"2pDNXUkBaRgI"},"source":["# LR.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDbj9ysaqW0w"},"source":["# SVM.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iT7fRz6irAKi"},"source":["# RFC.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c46jgOGisIdN"},"source":["# Guandando resultados\n","LR.to_csv('/content/drive/My Drive/Colab Notebooks/Factored project/models/results_RL.csv')\n","SVM.to_csv('/content/drive/My Drive/Colab Notebooks/Factored project/models/results_SVM.csv')\n","RFC.to_csv('/content/drive/My Drive/Colab Notebooks/Factored project/models/results_RFC.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dn0Ix7oTUj_x","colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1618107518326,"user_tz":300,"elapsed":837,"user":{"displayName":"Johan Estiben Lopez Cifuentes","photoUrl":"","userId":"00114855385701678854"}},"outputId":"b964608d-65e5-4a12-9f98-dbf30668a769"},"source":["LR = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Factored project/models/results_RL.csv')\n","LR.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Embedding</th>\n","      <th>text</th>\n","      <th>NoRuido</th>\n","      <th>PRE</th>\n","      <th>URLS</th>\n","      <th>HTML</th>\n","      <th>EMOJIS</th>\n","      <th>TODO</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>BOW</td>\n","      <td>0.770465</td>\n","      <td>0.767886</td>\n","      <td>0.758509</td>\n","      <td>0.767886</td>\n","      <td>0.770465</td>\n","      <td>0.770465</td>\n","      <td>0.758509</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>TF IDF</td>\n","      <td>0.751055</td>\n","      <td>0.754181</td>\n","      <td>0.738832</td>\n","      <td>0.754181</td>\n","      <td>0.751055</td>\n","      <td>0.751055</td>\n","      <td>0.738832</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Tensorflow hub</td>\n","      <td>0.782541</td>\n","      <td>0.779688</td>\n","      <td>0.765759</td>\n","      <td>0.779688</td>\n","      <td>0.782541</td>\n","      <td>0.782541</td>\n","      <td>0.765759</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0       Embedding      text  ...      HTML    EMOJIS      TODO\n","0  0           BOW             0.770465  ...  0.770465  0.770465  0.758509\n","1  1           TF IDF          0.751055  ...  0.751055  0.751055  0.738832\n","2  2           Tensorflow hub  0.782541  ...  0.782541  0.782541  0.765759\n","\n","[3 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"h-98-GMNp8H0","colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1618107528811,"user_tz":300,"elapsed":508,"user":{"displayName":"Johan Estiben Lopez Cifuentes","photoUrl":"","userId":"00114855385701678854"}},"outputId":"97515d4d-ff95-4bdb-8911-5afb153f94c2"},"source":["SVM = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Factored project/models/results_SVM.csv')\n","SVM.head()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Embedding</th>\n","      <th>text</th>\n","      <th>NoRuido</th>\n","      <th>PRE</th>\n","      <th>URLS</th>\n","      <th>HTML</th>\n","      <th>EMOJIS</th>\n","      <th>TODO</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>BOW</td>\n","      <td>0.739796</td>\n","      <td>0.744226</td>\n","      <td>0.749153</td>\n","      <td>0.744226</td>\n","      <td>0.739796</td>\n","      <td>0.739796</td>\n","      <td>0.749153</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>TF IDF</td>\n","      <td>0.746781</td>\n","      <td>0.743302</td>\n","      <td>0.730803</td>\n","      <td>0.743302</td>\n","      <td>0.746781</td>\n","      <td>0.746781</td>\n","      <td>0.730803</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Tensorflow hub</td>\n","      <td>0.786992</td>\n","      <td>0.785366</td>\n","      <td>0.785829</td>\n","      <td>0.785366</td>\n","      <td>0.786992</td>\n","      <td>0.786992</td>\n","      <td>0.785829</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0       Embedding      text  ...      HTML    EMOJIS      TODO\n","0  0           BOW             0.739796  ...  0.739796  0.739796  0.749153\n","1  1           TF IDF          0.746781  ...  0.746781  0.746781  0.730803\n","2  2           Tensorflow hub  0.786992  ...  0.786992  0.786992  0.785829\n","\n","[3 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"BERRtcfLVpJa","colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1618107534462,"user_tz":300,"elapsed":465,"user":{"displayName":"Johan Estiben Lopez Cifuentes","photoUrl":"","userId":"00114855385701678854"}},"outputId":"cb057a40-f78a-4df1-dcea-b597d9526f55"},"source":["RFC = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Factored project/models/results_RFC.csv')\n","RFC.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Embedding</th>\n","      <th>text</th>\n","      <th>NoRuido</th>\n","      <th>PRE</th>\n","      <th>URLS</th>\n","      <th>HTML</th>\n","      <th>EMOJIS</th>\n","      <th>TODO</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>BOW</td>\n","      <td>0.722320</td>\n","      <td>0.734908</td>\n","      <td>0.731070</td>\n","      <td>0.734908</td>\n","      <td>0.722320</td>\n","      <td>0.722320</td>\n","      <td>0.731070</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>TF IDF</td>\n","      <td>0.708520</td>\n","      <td>0.732517</td>\n","      <td>0.725237</td>\n","      <td>0.732517</td>\n","      <td>0.708520</td>\n","      <td>0.708520</td>\n","      <td>0.725237</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Tensorflow hub</td>\n","      <td>0.761269</td>\n","      <td>0.765101</td>\n","      <td>0.742181</td>\n","      <td>0.765101</td>\n","      <td>0.761269</td>\n","      <td>0.761269</td>\n","      <td>0.742181</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0       Embedding      text  ...      HTML    EMOJIS      TODO\n","0  0           BOW             0.722320  ...  0.722320  0.722320  0.731070\n","1  1           TF IDF          0.708520  ...  0.708520  0.708520  0.725237\n","2  2           Tensorflow hub  0.761269  ...  0.761269  0.761269  0.742181\n","\n","[3 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"htgp_nK0Vv8V"},"source":[""],"execution_count":null,"outputs":[]}]}